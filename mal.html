<!DOCTYPE html>
<html lang="ml">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; font-src 'self' *; media-stream 'self'; connect-src 'self' https://libretranslate.com;">
  <title>Credit Assessment - Malayalam</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <div class="intro-screen">
    <h1>‡¥ï‡µç‡¥∞‡µÜ‡¥°‡¥ø‡¥±‡µç‡¥±‡µç ‡¥Ö‡¥∏‡µÜ‡¥∏‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç</h1>
    <p>‡¥®‡¥Æ‡µÅ‡¥ï‡µç‡¥ï‡µç ‡¥§‡µÅ‡¥ü‡¥ô‡µç‡¥ô‡¥æ‡¥Ç...</p>
  </div>

  <div class="main-container">
    <div class="controls">
      <button onclick="window.location.href='index.html'">X</button>
    </div>
    <h2>‡¥ï‡µç‡¥∞‡µÜ‡¥°‡¥ø‡¥±‡µç‡¥±‡µç ‡¥Ö‡¥∏‡µÜ‡¥∏‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç ‡¥Ö‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥®‡µç‡¥±‡µç</h2>
    <video id="video" autoplay muted playsinline></video>
    <canvas id="visualizer"></canvas>

    <div class="chat-container" id="chatContainer"></div>

    <div class="chat-input" id="chatInputSection">
      <div class="chat-input-container">
        <input type="text" id="chatInput" placeholder="‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Æ‡¥±‡µÅ‡¥™‡¥ü‡¥ø ‡¥ü‡µà‡¥™‡µç‡¥™‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï (‡¥ì‡¥™‡µç‡¥∑‡¥£‡µΩ)">
        <button id="micBtn">üéôÔ∏è</button>
        <button id="rehearBtn">üîÑ</button>
      </div>
    </div>
  </div>

  <script>
    const canvas = document.getElementById('visualizer');
    const ctx = canvas.getContext('2d');
    canvas.width = canvas.offsetWidth;
    canvas.height = canvas.offsetHeight;

    let analyser, dataArray, audioContext, source;
    let audioStream, animationId;
    let recognition;
    let isSpeaking = false;
    let lastTranscript = '';
    let currentQuestionIndex = 0;
    let responses = [];
    let isAsking = false;
    let audioElement = new Audio();
    
    // Voice analysis variables
    let voiceConfidenceScores = [];
    let mediaRecorder;
    let audioChunks = [];

    const questions = [
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Æ‡µÅ‡¥¥‡µÅ‡¥µ‡µª ‡¥™‡µá‡¥∞‡µç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥®‡¥ø‡¥≤‡¥µ‡¥ø‡¥≤‡µÜ ‡¥§‡µä‡¥¥‡¥ø‡µΩ ‡¥∏‡µç‡¥•‡¥ø‡¥§‡¥ø ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥µ‡¥æ‡µº‡¥∑‡¥ø‡¥ï ‡¥µ‡¥∞‡µÅ‡¥Æ‡¥æ‡¥®‡¥Ç ‡¥é‡¥§‡µç‡¥∞‡¥Ø‡¥æ‡¥£‡µç?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥®‡¥ø‡¥≤‡¥µ‡¥ø‡¥≤‡µÅ‡¥≥‡µç‡¥≥ ‡¥µ‡¥æ‡¥Ø‡µç‡¥™‡¥ï‡¥≥‡µã ‡¥ï‡¥ü‡¥ô‡µç‡¥ô‡¥≥‡µã ‡¥â‡¥£‡µç‡¥ü‡µã?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥ï‡µç‡¥∞‡µÜ‡¥°‡¥ø‡¥±‡µç‡¥±‡µç ‡¥∏‡µç‡¥ï‡µã‡µº ‡¥Ö‡¥±‡¥ø‡¥Ø‡¥æ‡¥Æ‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥™‡¥±‡¥Ø‡µÅ‡¥ï?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥®‡µç‡¥® ‡¥ï‡µç‡¥∞‡µÜ‡¥°‡¥ø‡¥±‡µç‡¥±‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥â‡¥¶‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?",
      "‡¥é‡¥§‡µç‡¥∞ ‡¥§‡µÅ‡¥ï ‡¥ï‡µç‡¥∞‡µÜ‡¥°‡¥ø‡¥±‡µç‡¥±‡µç ‡¥Ü‡¥£‡µç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥®‡µç‡¥®‡¥§‡µç?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥®‡¥ø‡¥≤‡¥µ‡¥ø‡µΩ ‡¥§‡¥æ‡¥Æ‡¥∏‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥µ‡µÄ‡¥ü‡µç ‡¥∏‡µç‡¥µ‡¥®‡µç‡¥§‡¥Æ‡¥æ‡¥£‡µã ‡¥Ö‡¥§‡µã ‡¥µ‡¥æ‡¥ü‡¥ï‡¥Ø‡µç‡¥ï‡µç‡¥ï‡¥æ‡¥£‡µã?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥®‡¥ø‡¥≤‡¥µ‡¥ø‡¥≤‡µÜ ‡¥ú‡µã‡¥≤‡¥ø‡¥Ø‡¥ø‡µΩ ‡¥é‡¥§‡µç‡¥∞ ‡¥ï‡¥æ‡¥≤‡¥Æ‡¥æ‡¥Ø‡¥ø?",
      "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥Æ‡¥±‡µç‡¥±‡µç ‡¥µ‡¥∞‡µÅ‡¥Æ‡¥æ‡¥® ‡¥Æ‡¥æ‡µº‡¥ó‡¥ô‡µç‡¥ô‡µæ ‡¥â‡¥£‡µç‡¥ü‡µã?"
    ];

    if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
      recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.lang = 'ml-IN';
      recognition.continuous = true;
      recognition.interimResults = true;
    } else {
      appendMessage("‡¥à ‡¥¨‡µç‡¥∞‡µó‡¥∏‡¥±‡¥ø‡µΩ ‡¥∏‡µç‡¥™‡µÄ‡¥ö‡µç‡¥ö‡µç ‡¥±‡µÜ‡¥ï‡µç‡¥ï‡¥ó‡µç‡¥®‡¥ø‡¥∑‡µª ‡¥™‡¥ø‡¥®‡µç‡¥§‡µÅ‡¥£‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µç‡¥≤. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø Google Chrome ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ Edge ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï.", 'error');
    }

    if ('speechSynthesis' in window) {
      window.speechSynthesis.onvoiceschanged = () => {
        const voices = window.speechSynthesis.getVoices();
        console.log("Available voices:", voices.map(v => `${v.name} (${v.lang})`));
      };
    } else {
      appendMessage("‡¥ü‡µÜ‡¥ï‡µç‡¥∏‡µç‡¥±‡µç‡¥±‡µç-‡¥ü‡µÅ-‡¥∏‡µç‡¥™‡µÄ‡¥ö‡µç‡¥ö‡µç ‡¥à ‡¥¨‡µç‡¥∞‡µó‡¥∏‡¥±‡¥ø‡µΩ ‡¥≤‡¥≠‡µç‡¥Ø‡¥Æ‡¥≤‡µç‡¥≤.", 'error');
    }

    // Voice recording functions
    async function startVoiceRecording() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(stream);
        audioChunks = [];
        
        mediaRecorder.ondataavailable = (event) => {
          audioChunks.push(event.data);
        };
        
        mediaRecorder.start();
        console.log("Voice recording started");
      } catch (error) {
        console.error("Error starting voice recording:", error);
      }
    }

    function stopVoiceRecording() {
      return new Promise((resolve) => {
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
          mediaRecorder.onstop = async () => {
            const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
            const confidenceScore = await analyzeVoiceTone(audioBlob);
            voiceConfidenceScores.push(confidenceScore);
            console.log(`Voice confidence score: ${confidenceScore}`);
            resolve(confidenceScore);
          };
          mediaRecorder.stop();
        } else {
          resolve(0.5); // Default confidence if recording failed
        }
      });
    }

    async function analyzeVoiceTone(audioBlob) {
      try {
        // Convert audio to base64
        const arrayBuffer = await audioBlob.arrayBuffer();
        const base64Audio = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));
        
        // Send to Flask API for analysis
        const response = await fetch('/api/analyze-voice', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            audio_data: base64Audio,
            audio_format: 'wav'
          })
        });
        
        if (response.ok) {
          const result = await response.json();
          return result.confidence_score;
        } else {
          console.error('Voice analysis failed:', response.statusText);
          return 0.5; // Default confidence
        }
      } catch (error) {
        console.error('Error analyzing voice tone:', error);
        return 0.5; // Default confidence
      }
    }

    // Translation function using LibreTranslate
    async function translateToEnglish(malayalamText) {
      try {
        const response = await fetch('https://libretranslate.com/translate', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            q: malayalamText,
            source: 'ml',
            target: 'en'
          })
        });
        
        if (response.ok) {
          const result = await response.json();
          console.log(`Translated: "${malayalamText}" ‚Üí "${result.translatedText}"`);
          return result.translatedText;
        } else {
          console.error('Translation failed:', response.statusText);
          return malayalamText; // Return original if translation fails
        }
      } catch (error) {
        console.error('Error translating text:', error);
        return malayalamText; // Return original if translation fails
      }
    }

    function visualize() {
      analyser.getByteTimeDomainData(dataArray);
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      const gradient = ctx.createLinearGradient(0, 0, canvas.width, 0);
      gradient.addColorStop(0, '#4a4aff');
      gradient.addColorStop(1, '#8a8aff');
      ctx.beginPath();
      ctx.lineWidth = 2;
      ctx.strokeStyle = gradient;

      let sliceWidth = canvas.width * 1.0 / dataArray.length;
      let x = 0;
      for (let i = 0; i < dataArray.length; i++) {
        let v = dataArray[i] / 128.0;
        let y = v * canvas.height / 2;
        if (i === 0) {
          ctx.moveTo(x, y);
        } else {
          ctx.lineTo(x, y);
        }
        x += sliceWidth;
      }
      ctx.lineTo(canvas.width, canvas.height / 2);
      ctx.stroke();
      animationId = requestAnimationFrame(visualize);
    }

    async function speakQuestion(text) {
      return new Promise((resolve) => {
        isAsking = true;
        // Stop recognition while asking question
        if (recognition && isSpeaking) {
          recognition.stop();
        }
        
        const questionNumber = currentQuestionIndex + 1;
        const audioFile = questionNumber === 1 ? 'audio/malayalam/q1malcompress.m4a' : `audio/malayalam/q${questionNumber}.m4a`;
        console.log("Loading audio file:", audioFile);
        
        // Reset audio element
        audioElement.pause();
        audioElement.currentTime = 0;
        audioElement.src = audioFile;
        
        audioElement.onloadeddata = () => {
          console.log("Audio file loaded successfully");
        };

        audioElement.onended = () => {
          console.log("Audio playback completed");
          // Add 1 second buffer before accepting input
          setTimeout(() => {
            isAsking = false;
            console.log("Now accepting input");
            // Restart recognition after question is done
            if (recognition && !isSpeaking) {
              recognition.start();
            }
            resolve();
          }, 1000);
        };
        
        audioElement.onerror = (error) => {
          console.error("Audio playback error:", error);
          console.error("Audio error code:", audioElement.error ? audioElement.error.code : "unknown");
          console.error("Audio error message:", audioElement.error ? audioElement.error.message : "unknown");
          
          // Fall back to text-to-speech
          console.log("Falling back to text-to-speech");
          const utterance = new SpeechSynthesisUtterance(text);
          utterance.lang = 'ml-IN';
          const voices = window.speechSynthesis.getVoices();
          utterance.voice = voices.find(voice => voice.lang === 'ml-IN') || voices[0];
          utterance.rate = 0.8;
          
          utterance.onend = () => {
            console.log("TTS completed");
            setTimeout(() => {
              isAsking = false;
              console.log("Now accepting input");
              // Restart recognition after question is done
              if (recognition && !isSpeaking) {
                recognition.start();
              }
              resolve();
            }, 500);
          };
          
          utterance.onerror = (event) => {
            console.error("TTS error:", event);
            isAsking = false;
            resolve();
          };
          
          window.speechSynthesis.speak(utterance);
        };
        
        // Try to play the audio
        const playPromise = audioElement.play();
        if (playPromise !== undefined) {
          playPromise.catch(error => {
            console.error("Error playing audio:", error);
            audioElement.onerror(error);
          });
        }
      });
    }

    function startListening() {
      navigator.mediaDevices.getUserMedia({ audio: true, video: true })
        .then(stream => {
          audioStream = stream;
          document.getElementById('video').srcObject = stream;

          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          analyser = audioContext.createAnalyser();
          analyser.fftSize = 2048;
          dataArray = new Uint8Array(analyser.fftSize);

          source = audioContext.createMediaStreamSource(stream);
          source.connect(analyser);
          visualize();

          if (recognition) {
            try {
              recognition.start();
              console.log("Speech recognition started...");
              isSpeaking = true;
              document.getElementById('micBtn').textContent = 'üî¥';
            } catch (error) {
              console.error("Error starting recognition:", error);
              appendMessage("‡¥∏‡µç‡¥™‡µÄ‡¥ö‡µç‡¥ö‡µç ‡¥±‡µÜ‡¥ï‡µç‡¥ï‡¥ó‡µç‡¥®‡¥ø‡¥∑‡µª ‡¥Ü‡¥∞‡¥Ç‡¥≠‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡µΩ ‡¥™‡¥ø‡¥∂‡¥ï‡µç: " + error.message, 'error');
            }
          }
        })
        .catch(err => {
          console.error("Error accessing media devices:", err);
          if (err.name === "NotAllowedError" || err.name === "PermissionDeniedError") {
            appendMessage("‡¥Æ‡µà‡¥ï‡µç‡¥∞‡µã‡¥´‡µã‡µ∫ ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥ï‡¥æ‡¥Æ‡¥±‡¥Ø‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µç ‡¥Ü‡¥ï‡µç‚Äå‡¥∏‡¥∏‡µç ‡¥®‡¥ø‡¥∑‡µá‡¥ß‡¥ø‡¥ö‡µç‡¥ö‡µÅ. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥¨‡µç‡¥∞‡µó‡¥∏‡µº ‡¥∏‡µÜ‡¥±‡µç‡¥±‡¥ø‡¥Ç‡¥ó‡µÅ‡¥ï‡¥≥‡¥ø‡µΩ ‡¥Ü‡¥ï‡µç‚Äå‡¥∏‡¥∏‡µç ‡¥Ö‡¥®‡µÅ‡¥µ‡¥¶‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï.", 'error');
          } else {
            appendMessage("‡¥Æ‡µà‡¥ï‡µç‡¥∞‡µã‡¥´‡µã‡µ∫ ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥ï‡¥æ‡¥Æ‡¥±‡¥Ø‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µç ‡¥Ü‡¥ï‡µç‚Äå‡¥∏‡¥∏‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥æ‡µª ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Æ‡µç‡¥™‡µã‡µæ ‡¥™‡¥ø‡¥∂‡¥ï‡µç ‡¥∏‡¥Ç‡¥≠‡¥µ‡¥ø‡¥ö‡µç‡¥ö‡µÅ: " + err.message, 'error');
          }
        });
    }

    function stopListening() {
      if (audioStream) {
        audioStream.getTracks().forEach(track => track.stop());
      }
      if (recognition) {
        try {
          recognition.stop();
          console.log("Speech recognition stopped...");
          isSpeaking = false;
          document.getElementById('micBtn').textContent = 'üéôÔ∏è';
        } catch (error) {
          console.error("Error stopping recognition:", error);
        }
      }
      if (audioElement) {
        audioElement.pause();
        audioElement.currentTime = 0;
      }
      cancelAnimationFrame(animationId);
    }

    async function calculateCreditScore(responses, voiceConfidenceScores) {
      let baseScore = 0;
      responses.forEach((response, index) => {
        response = response.toLowerCase();
        switch (index) {
          case 0: // Full name
            baseScore += response.length > 5 ? 10 : 5;
            break;
          case 1: // Employment status
            baseScore += response.includes('employed') || response.includes('self-employed') ? 10 : 5;
            break;
          case 2: // Annual income
            const income = parseFloat(response.replace(/[^0-9.]/g, ''));
            baseScore += income > 0 ? Math.min(10, income / 10000) : 0;
            break;
          case 3: // Existing loans or debts
            baseScore += response.includes('no') ? 10 : 5;
            break;
          case 4: // Credit score
            const creditScore = parseInt(response);
            baseScore += creditScore > 300 ? Math.min(10, (creditScore - 300) / 50) : 0;
            break;
          case 5: // Purpose of credit
            baseScore += response.length > 10 ? 10 : 5;
            break;
          case 6: // Credit amount requested
            const amount = parseFloat(response.replace(/[^0-9.]/g, ''));
            baseScore += amount > 0 ? Math.min(10, amount / 10000) : 0;
            break;
          case 7: // Own or rent residence
            baseScore += response.includes('own') ? 10 : 5;
            break;
          case 8: // Duration at current job
            const duration = parseInt(response);
            baseScore += duration > 0 ? Math.min(10, duration / 2) : 0;
            break;
          case 9: // Additional income
            baseScore += response.includes('yes') ? 10 : 5;
            break;
        }
      });
      
      // Calculate voice confidence influence
      const averageVoiceConfidence = voiceConfidenceScores.length > 0 
        ? voiceConfidenceScores.reduce((a, b) => a + b, 0) / voiceConfidenceScores.length 
        : 0.5;
      
      // Voice multiplier: higher confidence = higher multiplier
      const voiceMultiplier = 0.8 + (averageVoiceConfidence * 0.4); // Range: 0.8 to 1.2
      
      // Final score with voice influence
      const finalScore = Math.min(100, Math.max(0, baseScore * voiceMultiplier));
      
      return {
        baseScore: Math.min(100, Math.max(0, baseScore)),
        voiceConfidence: averageVoiceConfidence,
        voiceMultiplier: voiceMultiplier,
        finalScore: finalScore
      };
    }

    async function askNextQuestion() {
      if (currentQuestionIndex < questions.length) {
        const question = `‡¥ö‡µã‡¥¶‡µç‡¥Ø‡¥Ç ${currentQuestionIndex + 1}: ${questions[currentQuestionIndex]}`;
        appendMessage(question, 'bot');
        console.log("Starting to ask question:", question);
        
        if (recognition && isSpeaking) {
          recognition.stop();
        }
        
        try {
          console.log("Attempting to speak question...");
          await speakQuestion(questions[currentQuestionIndex]);
          console.log("Finished speaking question");
        } catch (error) {
          console.error("Error speaking question:", error);
        }
        
        if (recognition && isSpeaking) {
          recognition.start();
        }
      } else {
        appendMessage("‡¥Ö‡¥∏‡µÜ‡¥∏‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡¥ø‡¥Ø‡¥æ‡¥Ø‡¥§‡¥ø‡¥®‡µç ‡¥®‡¥®‡µç‡¥¶‡¥ø. ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥â‡¥§‡µç‡¥§‡¥∞‡¥ô‡µç‡¥ô‡µæ ‡¥∞‡µá‡¥ñ‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡¥ø‡¥Ø‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ.", 'bot');
        stopListening();
        
        // Calculate credit score with voice analysis
        const scoreResult = await calculateCreditScore(responses, voiceConfidenceScores);
        console.log("Responses:", responses, "Score Result:", scoreResult);
        
        // Store all score components
        sessionStorage.setItem('finalScore', scoreResult.finalScore);
        sessionStorage.setItem('baseScore', scoreResult.baseScore);
        sessionStorage.setItem('voiceConfidence', scoreResult.voiceConfidence);
        sessionStorage.setItem('voiceMultiplier', scoreResult.voiceMultiplier);
        sessionStorage.setItem('assessmentResponses', JSON.stringify(responses));
        
        // Add View Results button
        const chatInputSection = document.getElementById('chatInputSection');
        chatInputSection.innerHTML = '<button id="viewResultsBtn" class="action-btn">‡¥´‡¥≤‡¥ô‡µç‡¥ô‡µæ ‡¥ï‡¥æ‡¥£‡µÅ‡¥ï</button>';
        document.getElementById('viewResultsBtn').addEventListener('click', () => {
          const aadhaarNumber = sessionStorage.getItem('aadhaarNumber');
          if (aadhaarNumber) {
            sessionStorage.setItem('aadhaarNumber', aadhaarNumber);
          }
          window.location.href = 'results.html?lang=ml';
        });
      }
    }

    if (recognition) {
      recognition.onresult = function(event) {
        if (isAsking) {
          console.log("Question is being asked, ignoring speech input");
          return;
        }
        
        let interimTranscript = '';
        let finalTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        if (finalTranscript && lastTranscript !== finalTranscript) {
          appendMessage(finalTranscript, 'user');
          responses[currentQuestionIndex] = finalTranscript;
          lastTranscript = finalTranscript;
          
          // Start voice recording for analysis
          startVoiceRecording();
          
          // Stop recording after a short delay and analyze
          setTimeout(async () => {
            const confidenceScore = await stopVoiceRecording();
            console.log(`Question ${currentQuestionIndex + 1} voice confidence: ${confidenceScore}`);
            
            currentQuestionIndex++;
            lastTranscript = '';
            setTimeout(askNextQuestion, 1000);
          }, 2000); // Record for 2 seconds
        }
      };

      recognition.onerror = function(event) {
        console.error("Speech recognition error:", event.error);
        appendMessage("‡¥∏‡µç‡¥™‡µÄ‡¥ö‡µç‡¥ö‡µç ‡¥±‡µÜ‡¥ï‡µç‡¥ï‡¥ó‡µç‡¥®‡¥ø‡¥∑‡µª ‡¥™‡¥ø‡¥∂‡¥ï‡µç: " + event.error, 'error');
        isSpeaking = false;
      };

      recognition.onend = function() {
        console.log("Speech recognition ended.");
        if (isSpeaking && currentQuestionIndex < questions.length) {
          setTimeout(() => {
            recognition.start();
          }, 500);
        }
      };
    }

    function appendMessage(text, sender) {
      const chatContainer = document.getElementById('chatContainer');
      const msg = document.createElement('div');
      msg.classList.add('chat-message', sender + '-message');
      msg.innerText = text;
      chatContainer.appendChild(msg);
      chatContainer.scrollTop = chatContainer.scrollHeight;
    }

    function handleTextInput() {
      const input = document.getElementById('chatInput').value.trim();
      if (!input || currentQuestionIndex >= questions.length) return;
      appendMessage(input, 'user');
      responses[currentQuestionIndex] = input;
      currentQuestionIndex++;
      document.getElementById('chatInput').value = '';
      lastTranscript = '';
      setTimeout(askNextQuestion, 1000);
    }

    const micBtn = document.getElementById('micBtn');
    const rehearBtn = document.getElementById('rehearBtn');
    const chatContainer = document.getElementById('chatContainer');
    const chatInput = document.getElementById('chatInput');

    micBtn.addEventListener('click', () => {
      if (!audioStream) {
        startListening();
        setTimeout(askNextQuestion, 1000);
      } else {
        stopListening();
      }
    });

    rehearBtn.addEventListener('click', () => {
      if (currentQuestionIndex < questions.length) {
        const question = `‡¥ö‡µã‡¥¶‡µç‡¥Ø‡¥Ç ${currentQuestionIndex + 1}: ${questions[currentQuestionIndex]}`;
        appendMessage("‡¥µ‡µÄ‡¥£‡µç‡¥ü‡µÅ‡¥Ç: " + question, 'bot');
        speakQuestion(questions[currentQuestionIndex]).catch(error => {
          console.error("Error replaying question:", error);
          appendMessage("‡¥ö‡µã‡¥¶‡µç‡¥Ø‡¥Ç ‡¥µ‡µÄ‡¥£‡µç‡¥ü‡µÅ‡¥Ç ‡¥™‡¥±‡¥Ø‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡µΩ ‡¥™‡¥ø‡¥∂‡¥ï‡µç - ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥µ‡µÄ‡¥£‡µç‡¥ü‡µÅ‡¥Ç ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï", 'error');
        });
      }
    });

    chatInput.addEventListener('keypress', (e) => {
      if (e.key === 'Enter') {
        handleTextInput();
      }
    });

    // Start the assessment when the page loads
    window.addEventListener('load', () => {
      const introScreen = document.querySelector('.intro-screen');
      introScreen.addEventListener('animationend', () => {
        introScreen.style.display = 'none';
        const mainContainer = document.querySelector('.main-container');
        mainContainer.style.opacity = 1;
        // Start asking questions after intro screen fades out
        setTimeout(askNextQuestion, 1000);
      });
    });
  </script>
</body>
</html>